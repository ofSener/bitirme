in many, many variations in the real world and this paper
only addresses the most common, mainstream versions.
It is rare that absolute statements can be made about this
technology, thus the qualiﬁers.
Document Structure
This document is mostly for software developers. It does
not go into enough technical details of the hardware to be
useful for hardware-oriented readers. But before we can
go into the practical information for developers a lot of
groundwork must be laid.
To that end, the second section describes random-access
memory (RAM) in technical detail. This section’s con-
tent is nice to know but not absolutely critical to be able
to understand the later sections. Appropriate back refer-
ences to the section are added in places where the content
is required so that the anxious reader could skip most of
this section at ﬁrst.
The third section goes into a lot of details of CPU cache
behavior. Graphs have been used to keep the text from
being as dry as it would otherwise be. This content is es-
sential for an understanding of the rest of the document.
Section 4 describes brieﬂy how virtual memory is imple-
mented. This is also required groundwork for the rest.
Section 5 goes into a lot of detail about Non Uniform
Memory Access (NUMA) systems.
Section 6 is the central section of this paper. It brings to-
gether all the previous sections’ information and gives
programmers advice on how to write code which per-
forms well in the various situations. The very impatient
reader could start with this section and, if necessary, go
back to the earlier sections to freshen up the knowledge
of the underlying technology.
Section 7 introduces tools which can help the program-
mer do a better job. Even with a complete understanding
of the technology it is far from obvious where in a non-
trivial software project the problems are. Some tools are
necessary.
In section 8 we ﬁnally give an outlook of technology
which can be expected in the near future or which might
just simply be good to have.
Reporting Problems
The author intends to update this document for some
time. This includes updates made necessary by advances
in technology but also to correct mistakes. Readers will-
ing to report problems are encouraged to send email to
the author. They are asked to include exact version in-
formation in the report. The version information can be
found on the last page of the document.
Thanks
I would like to thank Johnray Fuller and the crew at LWN
(especially Jonathan Corbet for taking on the daunting
task of transforming the author’s form of English into
something more traditional. Markus Armbruster provided
a lot of valuable input on problems and omissions in the
text.
About this Document
The title of this paper is an homage to David Goldberg’s
classic paper “What Every Computer Scientist Should
Know About Floating-Point Arithmetic” [12]. This pa-
per is still not widely known, although it should be a
prerequisite for anybody daring to touch a keyboard for
serious programming.
One word on the PDF: xpdf draws some of the diagrams
rather poorly. It is recommended it be viewed with evince
or, if really necessary, Adobe’s programs. If you use
evince be advised that hyperlinks are used extensively
throughout the document even though the viewer does
not indicate them like others do.
2 Version 1.0 What Every Programmer Should Know About Memory
2 Commodity Hardware Today
It is important to understand commodity hardware be-
cause specialized hardware is in retreat. Scaling these
days is most often achieved horizontally instead of verti-
cally, meaning today it is more cost-effective to use many
smaller, connected commodity computers instead of a
few really large and exceptionally fast (and expensive)
systems. This is the case because fast and inexpensive
network hardware is widely available. There are still sit-
uations where the large specialized systems have their
place and these systems still provide a business opportu-
nity, but the overall market is dwarfed by the commodity
hardware market. Red Hat, as of 2007, expects that for
future products, the “standard building blocks” for most
data centers will be a computer with up to four sockets,
each ﬁlled with a quad core CPU that, in the case of Intel
CPUs, will be hyper-threaded.2 This means the standard
system in the data center will have up to 64 virtual pro-
cessors. Bigger machines will be supported, but the quad
socket, quad CPU core case is currently thought to be the
sweet spot and most optimizations are targeted for such
machines.
Large differences exist in the structure of computers built
of commodity parts. That said, we will cover more than
90% of such hardware by concentrating on the most im-
portant differences. Note that these technical details tend
to change rapidly, so the reader is advised to take the date
of this writing into account.
Over the years personal computers and smaller servers
standardized on a chipset with two parts: the Northbridge
and Southbridge. Figure 2.1 shows this structure.
SouthbridgePCI-E SATA
USB
NorthbridgeRAM
CPU1 CPU2
FSB
Figure 2.1: Structure with Northbridge and Southbridge
All CPUs (two in the previous example, but there can be
more) are connected via a common bus (the Front Side
Bus, FSB) to the Northbridge. The Northbridge contains,
among other things, the memory controller, and its im-
plementation determines the type of RAM chips used for
the computer. Different types of RAM, such as DRAM,
Rambus, and SDRAM, require different memory con-
trollers.
To reach all other system devices, the Northbridge must
communicate with the Southbridge. The Southbridge,
often referred to as the I/O bridge, handles communica-
2Hyper-threading enables a single processor core to be used for two
or more concurrent executions with just a little extra hardware.
tion with devices through a variety of different buses. To-
day the PCI, PCI Express, SATA, and USB buses are of
most importance, but PATA, IEEE 1394, serial, and par-
allel ports are also supported by the Southbridge. Older
systems had AGP slots which were attached to the North-
bridge. This was done for performance reasons related to
insufﬁciently fast connections between the Northbridge
and Southbridge. However, today the PCI-E slots are all
connected to the Southbridge.
Such a system structure has a number of noteworthy con-
sequences:
• All data communication from one CPU to another
must travel over the same bus used to communicate
with the Northbridge.
• All communication with RAM must pass through
the Northbridge.
• The RAM has only a single port. 3
• Communication between a CPU and a device at-
tached to the Southbridge is routed through the
Northbridge.
A couple of bottlenecks are immediately apparent in this
design. One such bottleneck involves access to RAM for
devices. In the earliest days of the PC, all communica-
tion with devices on either bridge had to pass through the
CPU, negatively impacting overall system performance.
To work around this problem some devices became ca-
pable of direct memory access (DMA). DMA allows de-
vices, with the help of the Northbridge, to store and re-
ceive data in RAM directly without the intervention of
the CPU (and its inherent performance cost). Today all
high-performance devices attached to any of the buses
can utilize DMA. While this greatly reduces the work-
load on the CPU, it also creates contention for the band-
width of the Northbridge as DMA requests compete with
RAM access from the CPUs. This problem, therefore,
must be taken into account.
A second bottleneck involves the bus from the North-
bridge to the RAM. The exact details of the bus depend
on the memory types deployed. On older systems there
is only one bus to all the RAM chips, so parallel ac-
cess is not possible. Recent RAM types require two sep-
arate buses (or channels as they are called for DDR2,
see page 8) which doubles the available bandwidth. The
Northbridge interleaves memory access across the chan-
nels. More recent memory technologies (FB-DRAM, for
instance) add more channels.
With limited bandwidth available, it is important for per-
formance to schedule memory access in ways that mini-
mize delays. As we will see, processors are much faster
3We will not discuss multi-port RAM in this document as this type
of RAM is not found in commodity hardware, at least not in places
where the programmer has access to it. It can be found in specialized
hardware such as network routers which depend on utmost speed.
Ulrich Drepper Version 1.0 3
and must wait to access memory, despite the use of CPU
caches. If multiple hyper-threads, cores, or processors
access memory at the same time, the wait times for mem-
ory access are even longer. This is also true for DMA
operations.
There is more to accessing memory than concurrency,
however. Access patterns themselves also greatly inﬂu-
ence the performance of the memory subsystem, espe-
cially with multiple memory channels. In section 2.2 we
wil cover more details of RAM access patterns.
On some more expensive systems, the Northbridge does
not actually contain the memory controller. Instead the
Northbridge can be connected to a number of external
memory controllers (in the following example, four of
them).
SouthbridgePCI-E SATA
USB
Northbridge
MC2RAM
MC1RAM
MC4 RAM
MC3 RAM
CPU1 CPU2
Figure 2.2: Northbridge with External Controllers
The advantage of this architecture is that more than one
memory bus exists and therefore total available band-
width increases. This design also supports more memory.
Concurrent memory access patterns reduce delays by si-
multaneously accessing different memory banks. This
is especially true when multiple processors are directly
connected to the Northbridge, as in Figure 2.2. For such
a design, the primary limitation is the internal bandwidth
of the Northbridge, which is phenomenal for this archi-
tecture (from Intel).4
Using multiple external memory controllers is not the
only way to increase memory bandwidth. One other in-
creasingly popular way is to integrate memory controllers
into the CPUs and attach memory to each CPU. This
architecture is made popular by SMP systems based on
AMD’s Opteron processor. Figure 2.3 shows such a sys-
tem. Intel will have support for the Common System In-
terface (CSI) starting with the Nehalem processors; this
is basically the same approach: an integrated memory
controller with the possibility of local memory for each
processor.
With an architecture like this there are as many memory
banks available as there are processors. On a quad-CPU
machine the memory bandwidth is quadrupled without
the need for a complicated Northbridge with enormous
bandwidth. Having a memory controller integrated into
the CPU has some additional advantages; we will not dig
4For completeness it should be mentioned that such a memory con-
troller arrangement can be used for other purposes such as “memory
RAID” which is useful in combination with hotplug memory.
CPU3 CPU4
CPU1 CPU2
RAM
RAM
RAM
RAM
SouthbridgePCI-E SATA
USB
Figure 2.3: Integrated Memory Controller
deeper into this technology here.
There are disadvantages to this architecture, too. First of
all, because the machine still has to make all the mem-
ory of the system accessible to all processors, the mem-
ory is not uniform anymore (hence the name NUMA -
Non-Uniform Memory Architecture - for such an archi-
tecture). Local memory (memory attached to a proces-
sor) can be accessed with the usual speed. The situation
is different when memory attached to another processor
is accessed. In this case the interconnects between the
processors have to be used. To access memory attached
to CPU2 from CPU1 requires communication across one
interconnect. When the same CPU accesses memory at-
tached to CPU4 two interconnects have to be crossed.
Each such communication has an associated cost. We
talk about “NUMA factors” when we describe the ex-
tra time needed to access remote memory. The example
architecture in Figure 2.3 has two levels for each CPU:
immediately adjacent CPUs and one CPU which is two
interconnects away. With more complicated machines
the number of levels can grow signiﬁcantly. There are
also machine architectures (for instance IBM’s x445 and
SGI’s Altix series) where there is more than one type
of connection. CPUs are organized into nodes; within a
node the time to access the memory might be uniform or
have only small NUMA factors. The connection between
nodes can be very expensive, though, and the NUMA
factor can be quite high.
Commodity NUMA machines exist today and will likely
play an even greater role in the future. It is expected that,
from late 2008 on, every SMP machine will use NUMA.
The costs associated with NUMA make it important to
recognize when a program is running on a NUMA ma-
chine. In section 5 we will discuss more machine archi-
tectures and some technologies the Linux kernel provides
for these programs.
Beyond the technical details described in the remainder
of this section, there are several additional factors which
inﬂuence the performance of RAM. They are not con-
trollable by software, which is why they are not covered
in this section. The interested reader can learn about
some of these factors in section 2.1. They are really only
needed to get a more complete picture of RAM technol-
ogy and possibly to make better decisions when purchas-
ing computers.
4 Version 1.0 What Every Programmer Should Know About Memory
The following two sections discuss hardware details at
the gate level and the access protocol between the mem-
ory controller and the DRAM chips. Programmers will
likely ﬁnd this information enlightening since these de-
tails explain why RAM access works the way it does. It
is optional knowledge, though, and the reader anxious to
get to topics with more immediate relevance for everyday
life can jump ahead to section 2.2.5.
2.1 RAM Types
There have been many types of RAM over the years and
each type varies, sometimes signiﬁcantly, from the other.
The older types are today really only interesting to the
historians. We will not explore the details of those. In-
stead we will concentrate on modern RAM types; we will
only scrape the surface, exploring some details which
are visible to the kernel or application developer through
their performance characteristics.
The ﬁrst interesting details are centered around the ques-
tion why there are different types of RAM in the same
machine. More speciﬁcally, why are there both static
RAM (SRAM5) and dynamic RAM (DRAM). The for-
mer is much faster and provides the same functionality.
Why is not all RAM in a machine SRAM? The answer
is, as one might expect, cost. SRAM is much more ex-
pensive to produce and to use than DRAM. Both these
cost factors are important, the second one increasing in
importance more and more. To understand these differ-
ences we look at the implementation of a bit of storage
for both SRAM and DRAM.
In the remainder of this section we will discuss some
low-level details of the implementation of RAM. We will
keep the level of detail as low as possible. To that end,
we will discuss the signals at a “logic level” and not at a
level a hardware designer would have to use. That level
of detail is unnecessary for our purpose here.
2.1.1 Static RAM
M1 M3
M2 M4
M5
M6
Vdd
BL BL
WL
Figure 2.4: 6-T Static RAM
Figure 2.4 shows the structure of a 6 transistor SRAM
cell. The core of this cell is formed by the four transistors
M1 to M4 which form two cross-coupled inverters. They
have two stable states, representing 0 and 1 respectively.
The state is stable as long as power on Vdd is available.
5In other contexts SRAM might mean “synchronous RAM”.
If access to the state of the cell is needed the word access
line WL is raised. This makes the state of the cell imme-
diately available for reading on BL and BL. If the cell
state must be overwritten the BL and BL lines are ﬁrst
set to the desired values and then WL is raised. Since the
outside drivers are stronger than the four transistors (M1
through M4) this allows the old state to be overwritten.
See [20] for a more detailed description of the way the
cell works. For the following discussion it is important
to note that
• one cell requires six transistors. There are variants
with four transistors but they have disadvantages.
• maintaining the state of the cell requires constant
power.
• the cell state is available for reading almost im-
mediately once the word access line WL is raised.
The signal is as rectangular (changing quickly be-
tween the two binary states) as other transistor-
controlled signals.
• the cell state is stable, no refresh cycles are needed.
There are other, slower and less power-hungry, SRAM
forms available, but those are not of interest here since
we are looking at fast RAM. These slow variants are
mainly interesting because they can be more easily used
in a system than dynamic RAM because of their simpler
interface.
2.1.2 Dynamic RAM
Dynamic RAM is, in its structure, much simpler than
static RAM. Figure 2.5 shows the structure of a usual
DRAM cell design. All it consists of is one transistor
and one capacitor. This huge difference in complexity of
course means that it functions very differently than static
RAM.
DL
AL
M C
Figure 2.5: 1-T Dynamic RAM
A dynamic RAM cell keeps its state in the capacitor C.
The transistor M is used to guard the access to the state.
To read the state of the cell the access line AL is raised;
this either causes a current to ﬂow on the data line DL or
not, depending on the charge in the capacitor. To write
to the cell the data line DL is appropriately set and then
AL is raised for a time long enough to charge or drain
the capacitor.
There are a number of complications with the design of
dynamic RAM. The use of a capacitor means that reading
Ulrich Drepper Version 1.0 5
the cell discharges the capacitor. The procedure cannot
be repeated indeﬁnitely, the capacitor must be recharged
at some point. Even worse, to accommodate the huge
number of cells (chips with 109 or more cells are now
common) the capacity to the capacitor must be low (in
the femto-farad range or lower). A fully charged capac-
itor holds a few 10’s of thousands of electrons. Even
though the resistance of the capacitor is high (a couple of
tera-ohms) it only takes a short time for the capacity to
dissipate. This problem is called “leakage”.
This leakage is why a DRAM cell must be constantly
refreshed. For most DRAM chips these days this refresh
must happen every 64ms. During the refresh cycle no
access to the memory is possible since a refresh is simply
a memory read operation where the result is discarded.
For some workloads this overhead might stall up to 50%
of the memory accesses (see [3]).
A second problem resulting from the tiny charge is that
the information read from the cell is not directly usable.
The data line must be connected to a sense ampliﬁer
which can distinguish between a stored 0 or 1 over the
whole range of charges which still have to count as 1.
A third problem is that reading a cell causes the charge
of the capacitor to be depleted. This means every read
operation must be followed by an operation to recharge
the capacitor. This is done automatically by feeding the
output of the sense ampliﬁer back into the capacitor. It
does mean, though, the reading memory content requires
additional energy and, more importantly, time.
A fourth problem is that charging and draining a capac-
itor is not instantaneous. The signals received by the
sense ampliﬁer are not rectangular, so a conservative es-
timate as to when the output of the cell is usable has to
be used. The formulas for charging and discharging a
capacitor are
QCharge(t) = Q0(1 − e− t
RC )
QDischarge(t) = Q0e− t
RC
This means it takes some time (determined by the capac-
ity C and resistance R) for the capacitor to be charged and
discharged. It also means that the current which can be
detected by the sense ampliﬁers is not immediately avail-
able. Figure 2.6 shows the charge and discharge curves.
The X–axis is measured in units of RC (resistance multi-
plied by capacitance) which is a unit of time.
Unlike the static RAM case where the output is immedi-
ately available when the word access line is raised, it will
always take a bit of time until the capacitor discharges
sufﬁciently. This delay severely limits how fast DRAM
can be.
The simple approach has its advantages, too. The main
advantage is size. The chip real estate needed for one
DRAM cell is many times smaller than that of an SRAM
1RC 2RC 3RC 4RC 5RC 6RC 7RC 8RC 9RC
0
10
20
30
40
50
60
70
80
90
100
Percentage Charge
Charge Discharge
Figure 2.6: Capacitor Charge and Discharge Timing
cell. The SRAM cells also need individual power for
the transistors maintaining the state. The structure of
the DRAM cell is also simpler and more regular which
means packing many of them close together on a die is
simpler.
Overall, the (quite dramatic) difference in cost wins. Ex-
cept in specialized hardware – network routers, for exam-
ple – we have to live with main memory which is based
on DRAM. This has huge implications on the program-
mer which we will discuss in the remainder of this paper.
But ﬁrst we need to look into a few more details of the
actual use of DRAM cells.
2.1.3 DRAM Access
A program selects a memory location using a virtual ad-
dress. The processor translates this into a physical ad-
dress and ﬁnally the memory controller selects the RAM
chip corresponding to that address. To select the individ-
ual memory cell on the RAM chip, parts of the physical
address are passed on in the form of a number of address
lines.
It would be completely impractical to address memory
locations individually from the memory controller: 4GB
of RAM would require 232 address lines. Instead the
address is passed encoded as a binary number using a
smaller set of address lines. The address passed to the
DRAM chip this way must be demultiplexed ﬁrst. A
demultiplexer with N address lines will have 2N output
lines. These output lines can be used to select the mem-
ory cell. Using this direct approach is no big problem for
chips with small capacities.
But if the number of cells grows this approach is not suit-
able anymore. A chip with 1Gbit6 capacity would need
30 address lines and 230 select lines. The size of a de-
multiplexer increases exponentially with the number of
input lines when speed is not to be sacriﬁced. A demulti-
plexer for 30 address lines needs a whole lot of chip real
estate in addition to the complexity (size and time) of
the demultiplexer. Even more importantly, transmitting
6I hate those SI preﬁxes. For me a giga-bit will always be 230 and
not 109 bits.
6 Version 1.0 What Every Programmer Should Know About Memory
30 impulses on the address lines synchronously is much
harder than transmitting “only” 15 impulses. Fewer lines
have to be laid out at exactly the same length or timed
appropriately.7
Row Address Selection
a0
a1
Column Address Selection
a2
a3
Data
Figure 2.7: Dynamic RAM Schematic
Figure 2.7 shows a DRAM chip at a very high level. The
DRAM cells are organized in rows and columns. They
could all be aligned in one row but then the DRAM chip
would need a huge demultiplexer. With the array ap-
proach the design can get by with one demultiplexer and
one multiplexer of half the size.8 This is a huge saving
on all fronts. In the example the address lines a0 and a1
through the row address selection (RAS)9 demultiplexer
select the address lines of a whole row of cells. When
reading, the content of all cells is thusly made available to
the column address selection (CAS)9 multiplexer. Based
on the address lines a2 and a3 the content of one col-
umn is then made available to the data pin of the DRAM
chip. This happens many times in parallel on a number
of DRAM chips to produce a total number of bits corre-
sponding to the width of the data bus.
For writing, the new cell value is put on the data bus and,
when the cell is selected using the RAS and CAS, it is
stored in the cell. A pretty straightforward design. There
are in reality – obviously – many more complications.
There need to be speciﬁcations for how much delay there
is after the signal before the data will be available on the
data bus for reading. The capacitors do not unload instan-
taneously, as described in the previous section. The sig-
nal from the cells is so weak that it needs to be ampliﬁed.
For writing it must be speciﬁed how long the data must
be available on the bus after the RAS and CAS is done to
successfully store the new value in the cell (again, capac-
7Modern DRAM types like DDR3 can automatically adjust the tim-
ing but there is a limit as to what can be tolerated.
8Multiplexers and demultiplexers are equivalent and the multiplexer
here needs to work as a demultiplexer when writing. So we will drop
the differentiation from now on.
9The line over the name indicates that the signal is negated.
itors do not ﬁll or drain instantaneously). These timing
constants are crucial for the performance of the DRAM
chip. We will talk about this in the next section.
A secondary scalability problem is that having 30 address
lines connected to every RAM chip is not feasible either.
Pins of a chip are precious resources. It is “bad” enough
that the data must be transferred as much as possible in
parallel (e.g., in 64 bit batches). The memory controller
must be able to address each RAM module (collection of
RAM chips). If parallel access to multiple RAM mod-
ules is required for performance reasons and each RAM
module requires its own set of 30 or more address lines,
then the memory controller needs to have, for 8 RAM
modules, a whopping 240+ pins only for the address han-
dling.
To counter these secondary scalability problems DRAM
chips have, for a long time, multiplexed the address it-
self. That means the address is transferred in two parts.
The ﬁrst part consisting of address bits (a0 and a1 in the
example in Figure 2.7) select the row. This selection re-
mains active until revoked. Then the second part, address
bits a2 and a3, select the column. The crucial difference
is that only two external address lines are needed. A few
more lines are needed to indicate when the RAS and CAS
signals are available but this is a small price to pay for
cutting the number of address lines in half. This address
multiplexing brings its own set of problems, though. We
will discuss them in section 2.2.
2.1.4 Conclusions
Do not worry if the details in this section are a bit over-
whelming. The important things to take away from this
section are:
• there are reasons why not all memory is SRAM
• memory cells need to be individually selected to
be used
• the number of address lines is directly responsi-
ble for the cost of the memory controller, mother-
boards, DRAM module, and DRAM chip
• it takes a while before the results of the read or
write operation are available
The following section will go into more details about the
actual process of accessing DRAM memory. We are not
going into more details of accessing SRAM, which is
usually directly addressed. This happens for speed and
because the SRAM memory is limited in size. SRAM
is currently used in CPU caches and on-die where the
connections are small and fully under control of the CPU
designer. CPU caches are a topic which we discuss later
but all we need to know is that SRAM cells have a certain
maximum speed which depends on the effort spent on the
SRAM. The speed can vary from only slightly slower
Ulrich Drepper Version 1.0 7
than the CPU core to one or two orders of magnitude
slower.
2.2 DRAM Access Technical Details
In the section introducing DRAM we saw that DRAM
chips multiplex the addresses in order to save resources
int the form of address pins. We also saw that access-
ing DRAM cells takes time since the capacitors in those
cells do not discharge instantaneously to produce a stable
signal; we also saw that DRAM cells must be refreshed.
Now it is time to put this all together and see how all
these factors determine how the DRAM access has to
happen.
We will concentrate on current technology; we will not
discuss asynchronous DRAM and its variants as they are
simply not relevant anymore. Readers interested in this
topic are referred to [3] and [19]. We will also not talk
about Rambus DRAM (RDRAM) even though the tech-
nology is not obsolete. It is just not widely used for sys-
tem memory. We will concentrate exclusively on Syn-
chronous DRAM (SDRAM) and its successors Double
Data Rate DRAM (DDR).
Synchronous DRAM, as the name suggests, works rel-
ative to a time source. The memory controller provides
a clock, the frequency of which determines the speed of
the Front Side Bus (FSB) – the memory controller in-
terface used by the DRAM chips. As of this writing,
frequencies of 800MHz, 1,066MHz, or even 1,333MHz
are available with higher frequencies (1,600MHz) being
announced for the next generation. This does not mean
the frequency used on the bus is actually this high. In-
stead, today’s buses are double- or quad-pumped, mean-
ing that data is transported two or four times per cy-
cle. Higher numbers sell so the manufacturers like to
advertise a quad-pumped 200MHz bus as an “effective”
800MHz bus.
For SDRAM today each data transfer consists of 64 bits
– 8 bytes. The transfer rate of the FSB is therefore 8
bytes multiplied by the effective bus frequency (6.4GB/s
for the quad-pumped 200MHz bus). That sounds a lot
but it is the burst speed, the maximum speed which will
never be surpassed. As we will see now the protocol for
talking to the RAM modules has a lot of downtime when
no data can be transmitted. It is exactly this downtime
which we must understand and minimize to achieve the
best performance.
2.2.1 Read Access Protocol
Figure 2.8 shows the activity on some of the connectors
of a DRAM module which happens in three differently
colored phases. As usual, time ﬂows from left to right.
A lot of details are left out. Here we only talk about the
bus clock, RAS and CAS signals, and the address and
data buses. A read cycle begins with the memory con-
troller making the row address available on the address
CLK
RAS
CAS
Row
Addr
Col
Addr
Address
Data
Out
Data
Out
Data
Out
Data
Out
DQ
tRCD CL
Figure 2.8: SDRAM Read Access Timing
bus and lowering the RAS signal. All signals are read on
the rising edge of the clock (CLK) so it does not matter if
the signal is not completely square as long as it is stable
at the time it is read. Setting the row address causes the
RAM chip to start latching the addressed row.
The CAS signal can be sent after tRCD (RAS-to-CAS
Delay) clock cycles. The column address is then trans-
mitted by making it available on the address bus and low-
ering the CAS line. Here we can see how the two parts
of the address (more or less halves, nothing else makes
sense) can be transmitted over the same address bus.
Now the addressing is complete and the data can be trans-
mitted. The RAM chip needs some time to prepare for
this. The delay is usually called CAS Latency (CL). In
Figure 2.8 the CAS latency is 2. It can be higher or lower,
depending on the quality of the memory controller, moth-
erboard, and DRAM module. The latency can also have
half values. With CL=2.5 the ﬁrst data would be avail-
able at the ﬁrst falling ﬂank in the blue area.
With all this preparation to get to the data it would be
wasteful to only transfer one data word. This is why
DRAM modules allow the memory controller to spec-
ify how much data is to be transmitted. Often the choice
is between 2, 4, or 8 words. This allows ﬁlling entire
lines in the caches without a new RAS/CAS sequence. It
is also possible for the memory controller to send a new
CAS signal without resetting the row selection. In this
way, consecutive memory addresses can be read from
or written to signiﬁcantly faster because the RAS sig-
nal does not have to be sent and the row does not have
to be deactivated (see below). Keeping the row “open”
is something the memory controller has to decide. Spec-
ulatively leaving it open all the time has disadvantages
with real-world applications (see [3]). Sending new CAS
signals is only subject to the Command Rate of the RAM
module (usually speciﬁed as Tx, where x is a value like
1 or 2; it will be 1 for high-performance DRAM modules
which accept new commands every cycle).
In this example the SDRAM spits out one word per cy-
cle. This is what the ﬁrst generation does. DDR is able
to transmit two words per cycle. This cuts down on the
transfer time but does not change the latency. In princi-
8 Version 1.0 What Every Programmer Should Know About Memory
ple, DDR2 works the same although in practice it looks
different. There is no need to go into the details here. It is
sufﬁcient to note that DDR2 can be made faster, cheaper,
more reliable, and is more energy efﬁcient (see [6] for
more information).
2.2.2 Precharge and Activation
Figure 2.8 does not cover the whole cycle. It only shows
parts of the full cycle of accessing DRAM. Before a new
RAS signal can be sent the currently latched row must be
deactivated and the new row must be precharged. We can
concentrate here on the case where this is done with an
explicit command. There are improvements to the pro-
tocol which, in some situations, allows this extra step to
be avoided. The delays introduced by precharging still
affect the operation, though.
CLK
RAS
tRP
CAS
WE
Col
Addr
Row
Addr
Col
Addr
Address
Data
Out
Data
Out
DQ
tRCD
CL
Figure 2.9: SDRAM Precharge and Activation
Figure 2.9 shows the activity starting from one CAS sig-
nal to the CAS signal for another row. The data requested
with the ﬁrst CAS signal is available as before, after CL
cycles. In the example two words are requested which,
on a simple SDRAM, takes two cycles to transmit. Al-
ternatively, imagine four words on a DDR chip.
Even on DRAM modules with a command rate of one
the precharge command cannot be issued right away. It
is necessary to wait as long as it takes to transmit the
data. In this case it takes two cycles. This happens to be
the same as CL but that is just a coincidence. The pre-
charge signal has no dedicated line; instead, some imple-
mentations issue it by lowering the Write Enable (WE)
and RAS line simultaneously. This combination has no
useful meaning by itself (see [18] for encoding details).
Once the precharge command is issued it takes tRP (Row
Precharge time) cycles until the row can be selected. In
Figure 2.9 much of the time (indicated by the purplish
color) overlaps with the memory transfer (light blue).
This is good! But tRP is larger than the transfer time
and so the next RAS signal is stalled for one cycle.
If we were to continue the timeline in the diagram we
would ﬁnd that the next data transfer happens 5 cycles
after the previous one stops. This means the data bus is
only in use two cycles out of seven. Multiply this with
the FSB speed and the theoretical 6.4GB/s for a 800MHz
bus become 1.8GB/s. That is bad and must be avoided.
The techniques described in section 6 help to raise this
number. But the programmer usually has to do her share.
There is one more timing value for a SDRAM module
which we have not discussed. In Figure 2.9 the precharge
command was only limited by the data transfer time. An-
other constraint is that an SDRAM module needs time
after a RAS signal before it can precharge another row
(denoted as tRAS). This number is usually pretty high,
in the order of two or three times the tRP value. This is
a problem if, after a RAS signal, only one CAS signal
follows and the data transfer is ﬁnished in a few cycles.
Assume that in Figure 2.9 the initial CAS signal was pre-
ceded directly by a RAS signal and that tRAS is 8 cycles.
Then the precharge command would have to be delayed
by one additional cycle since the sum of tRCD, CL, and
tRP (since it is larger than the data transfer time) is only
7 cycles.
DDR modules are often described using a special nota-
tion: w-x-y-z-T. For instance: 2-3-2-8-T1. This means:
w 2 CAS Latency (CL)
x 3 RAS-to-CAS delay (tRCD)
y 2 RAS Precharge (tRP)
z 8 Active to Precharge delay (tRAS)
T T1 Command Rate
There are numerous other timing constants which affect
the way commands can be issued and are handled. Those
ﬁve constants are in practice sufﬁcient to determine the
performance of the module, though.
It is sometimes useful to know this information for the
computers in use to be able to interpret certain measure-
ments. It is deﬁnitely useful to know these details when
buying computers since they, along with the FSB and
SDRAM module speed, are among the most important
factors determining a computer’s speed.
The very adventurous reader could also try to tweak a
system. Sometimes the BIOS allows changing some or
all these values. SDRAM modules have programmable
registers where these values can be set. Usually the BIOS
picks the best default value. If the quality of the RAM
module is high it might be possible to reduce the one
or the other latency without affecting the stability of the
computer. Numerous overclocking websites all around
the Internet provide ample of documentation for doing
this. Do it at your own risk, though and do not say you
have not been warned.
2.2.3 Recharging
A mostly-overlooked topic when it comes to DRAM ac-
cess is recharging. As explained in section 2.1.2, DRAM
cells must constantly be refreshed. This does not happen
Ulrich Drepper Version 1.0 9
completely transparently for the rest of the system. At
times when a row10 is recharged no access is possible.
The study in [3] found that “[s]urprisingly, DRAM re-
fresh organization can affect performance dramatically”.
Each DRAM cell must be refreshed every 64ms accord-
ing to the JEDEC (Joint Electron Device Engineering
Council) speciﬁcation. If a DRAM array has 8,192 rows
this means the memory controller has to issue a refresh
command on average every 7.8125µs (refresh commands
can be queued so in practice the maximum interval be-
tween two requests can be higher). It is the memory
controller’s responsibility to schedule the refresh com-
mands. The DRAM module keeps track of the address
of the last refreshed row and automatically increases the
address counter for each new request.
There is really not much the programmer can do about
the refresh and the points in time when the commands are
issued. But it is important to keep this part of the DRAM
life cycle in mind when interpreting measurements. If a
critical word has to be retrieved from a row which cur-
rently is being refreshed the processor could be stalled
for quite a long time. How lo