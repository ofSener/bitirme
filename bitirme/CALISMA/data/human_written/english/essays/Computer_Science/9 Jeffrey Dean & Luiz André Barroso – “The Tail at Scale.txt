Global resource sharing. Applica-
tions running on different machines
might contend for global resources
(such as network switches and shared
file systems);
Maintenance activities. Background
activities (such as data reconstruction
in distributed file systems, periodic log
compactions in storage systems like
BigTable,4 and periodic garbage collec-
tion in garbage-collected languages) can
cause periodic spikes in latency; and
Queueing. Multiple layers of queue-
ing in intermediate servers and network
switches amplify this variability.
Increased variability is also due to
several hardware trends:
Power limits. Modern CPUs are de-
signed to temporarily run above their
average power envelope, mitigating
thermal effects by throttling if this activ-
ity is sustained for a long period;5
Garbage collection. Solid-state stor-
age devices provide very fast random
read access, but the need to periodically
garbage collect a large number of data
blocks can increase read latency by a
factor of 100 with even a modest level of
write activity; and
Energy management. Power-saving
modes in many types of devices save
considerable energy but add additional
latency when moving from inactive to
active modes.
component-Level variability
amplified By Scale
A common technique for reducing la-
tency in large-scale online services is to
parallelize sub-operations across many
different machines, where each sub-op-
eration is co-located with its portion of
a large dataset. Parallelization happens
by fanning out a request from a root to
a large number of leaf servers and merg-
ing responses via a request-distribution
tree. These sub-operations must all
complete within a strict deadline for the
service to feel responsive.
Variability in the latency distribu-
tion of individual components is mag-
nified at the service level; for example,
consider a system where each server
typically responds in 10ms but with a
99th
-percentile latency of one second.
If a user request is handled on just one
such server, one user request in 100 will
be slow (one second). The figure here
outlines how service-level latency in this
hypothetical scenario is affected by very
modest fractions of latency outliers. If
a user request must collect responses
from 100 such servers in parallel, then
63% of user requests will take more than
one second (marked “x” in the figure).
Even for services with only one in 10,000
requests experiencing more than one-
second latencies at the single-server
level, a service with 2,000 such servers
will see almost one in five user requests
taking more than one second (marked
“o” in the figure).
Table 1 lists measurements from
a real Google service that is logically
similar to this idealized scenario; root
servers distribute a request through in-
termediate servers to a very large num-
ber of leaf servers. The table shows the
effect of large fan-out on latency distri-
butions. The 99th
-percentile latency for
a single random request to finish, mea-
sured at the root, is 10ms. However, the
99th
-percentile latency for all requests to
finish is 140ms, and the 99th
-percentile
latency for 95% of the requests finish-
ing is 70ms, meaning that waiting for
the slowest 5% of the requests to com-
plete is responsible for half of the total
99%-percentile latency. Techniques that
concentrate on these slow outliers can
yield dramatic reductions in overall ser-
vice performance.
Overprovisioning of resources, care-
ful real-time engineering of software,
and improved reliability can all be
used at all levels and in all components
to reduce the base causes of variability.
We next describe general approaches
useful for reducing variability in ser-
vice responsiveness.
Reducing component variability
Interactive response-time variability
can be reduced by ensuring interactive
requests are serviced in a timely manner
through many small engineering deci-
sions, including:
Differentiating service classes and
higher-level queuing. Differentiated ser-
vice classes can be used to prefer sched-
uling requests for which a user is wait-
ing over non-interactive requests. Keep
low-level queues short so higher-level
policies take effect more quickly; for ex-
ample, the storage servers in Google’s
cluster-level file-system software keep
few operations outstanding in the op-
erating system’s disk queue, instead
maintaining their own priority queues
of pending disk requests. This shallow
queue allows the servers to issue incom-
ing high-priority interactive requests
before older requests for latency-insen-
sitive batch operations are served.
Reducing head-of-line blocking. High-
level services can handle requests with
widely varying intrinsic costs. It is some-
times useful for the system to break
long-running requests into a sequence
of smaller requests to allow interleaving
of the execution of other short-running
requests; for example, Google’s Web
search system uses such time-slicing to
prevent a small number of very compu-
tationally expensive queries from add-
ing substantial latency to a large num-
ber of concurrent cheaper queries.
Managing background activities and
synchronized disruption. Background
tasks can create significant CPU, disk,
or network load; examples are log
compaction in log-oriented storage
systems and garbage-collector activity
in garbage-collected languages. A com-
bination of throttling, breaking down
heavyweight operations into smaller
operations, and triggering such opera-
tions at times of lower overall load is
often able to reduce the effect of back-
ground activities on interactive request
latency. For large fan-out services, it is
sometimes useful for the system to syn-
chronize the background activity across
many different machines. This synchro-
nization enforces a brief burst of activity
on each machine simultaneously, slow-
ing only those interactive requests being
handled during the brief period of back-
ground activity. In contrast, without syn-
chronization, a few machines are always
doing some background activity, push-
ing out the latency tail on all requests.
Missing in this discussion so far is
any reference to caching. While effec-
tive caching layers can be useful, even a
necessity in some systems, they do not
directly address tail latency, aside from
configurations where it is guaranteed
that the entire working set of an applica-
tion can reside in a cache.
Living with Latency variability
The careful engineering techniques in
the preceding section are essential for
building high-performance interactive
services, but the scale and complexity
of modern Web services make it infea-
sible to eliminate all latency variabil-
ity. Even if such perfect behavior could
be achieved in isolated environments,
contributed articles
february 2013 | vol. 56 | no. 2 | communicaTionS of The acm 77
systems with shared computational
resources exhibit performance fluctua-
tions beyond the control of application
developers. Google has therefore found
it advantageous to develop tail-tolerant
techniques that mask or work around
temporary latency pathologies, instead
of trying to eliminate them altogether.
We separate these techniques into two
main classes: The first corresponds to
within-request immediate-response
techniques that operate at a time scale
of tens of milliseconds, before longer-
term techniques have a chance to react.
The second consists of cross-request
long-term adaptations that perform on
a time scale of tens of seconds to min-
utes and are meant to mask the effect of
longer-term phenomena.
Within Request Short-Term
adaptations
A broad class of Web services deploy
multiple replicas of data items to pro-
vide additional throughput capacity and
maintain availability in the presence of
failures. This approach is particularly
effective when most requests operate on
largely read-only, loosely consistent da-
tasets; an example is a spelling-correc-
tion service that has its model updated
once a day while handling thousands of
correction requests per second. Simi-
larly, distributed file systems may have
multiple replicas of a given data chunk
that can all be used to service read re-
quests. The techniques here show how
replication can also be used to reduce
latency variability within a single high-
er-level request:
Hedged requests. A simple way to
curb latency variability is to issue the
same request to multiple replicas and
use the results from whichever replica
responds first. We term such requests
“hedged requests” because a client first
sends one request to the replica be-
lieved to be the most appropriate, but
then falls back on sending a secondary
request after some brief delay. The cli-
ent cancels remaining outstanding re-
quests once the first result is received.
Although naive implementations of
this technique typically add unaccept-
able additional load, many variations
exist that give most of the latency-re-
duction effects while increasing load
only modestly.
One such approach is to defer send-
ing a secondary request until the first
request has been outstanding for more
than the 95th
-percentile expected la-
tency for this class of requests. This
approach limits the additional load to
approximately 5% while substantially
shortening the latency tail. The tech-
nique works because the source of la-
tency is often not inherent in the par-
ticular request but rather due to other
forms of interference. For example, in
a Google benchmark that reads the val-
ues for 1,000 keys stored in a BigTable
table distributed across 100 different
servers, sending a hedging request after
a 10ms delay reduces the 99.9th
-percen-
tile latency for retrieving all 1,000 values
from 1,800ms to 74ms while sending
just 2% more requests. The overhead of
hedged requests can be further reduced
by tagging them as lower priority than
the primary requests.
Tied requests. The hedged-requests
technique also has a window of vulner-
ability in which multiple servers can
execute the same request unnecessar-
ily. That extra work can be capped by
waiting for the 95th-percentile expect-
ed latency before issuing the hedged
request, but this approach limits the
benefits to only a small fraction of re-
quests. Permitting more aggressive
use of hedged requests with moderate
resource consumption requires faster
cancellation of requests.
A common source of variability is
queueing delays on the server before
a request begins execution. For many
services, once a request is actually
scheduled and begins execution, the
variability of its completion time goes
down substantially. Mitzenmacher10
said allowing a client to choose between
two servers based on queue lengths at
enqueue time exponentially improves
load-balancing performance over a uni-
form random scheme. We advocate notchoosing but rather enqueuing copies
of a request in multiple servers simulta-
neously and allowing the servers to com-
municate updates on the status of these
copies to each other. We call requests
where servers perform cross-server sta-
tus updates “tied requests.” The sim-
plest form of a tied request has the cli-
ent send the request to two different
servers, each tagged with the identity of
the other server (“tied”). When a request
begins execution, it sends a cancellation
message to its counterpart. The corre-
sponding request, if still enqueued in
the other server, can be aborted imme-
diately or deprioritized substantially.
There is a brief window of one aver-
age network message delay where both
servers may start executing the request
while the cancellation messages are
both in flight to the other server. A com-
mon case where this situation can occur
is if both server queues are completely
empty. It is useful therefore for the cli-
ent to introduce a small delay of two
times the average network message de-
lay (1ms or less in modern data-center
networks) between sending the first re-
quest and sending the second request.
Google’s implementation of this
technique in the context of its cluster-
level distributed file system is effective
at reducing both median and tail laten-
cies. Table 2 lists the times for servicing
a small read request from a BigTable
where the data is not cached in memory
but must be read from the underlying
file system; each file chunk has three
replicas on distinct machines. The table
includes read latencies observed with
and without tied requests for two sce-
narios: The first is a cluster in which the
benchmark is running in isolation, in
which case latency variability is mostly
from self-interference and regular clus-
ter-management activities. In it, send-
ing a tied request that does cross-server
cancellation to another file system
replica following 1ms reduces median
latency by 16% and is increasingly ef-
fective along the tail of the latency dis-
tribution, achieving nearly 40% reduc-
tion at the 99.9th
-percentile latency. The
second scenario is like the first except
there is also a large, concurrent sorting
job running on the same cluster con-
tending for the same disk resources in
the shared file system. Although overall
latencies are somewhat higher due to
higher utilization, similar reductions in
the latency profile are achieved with the
tied-request technique discussed earli-
er. The latency profile with tied requests
while running a concurrent large sort-
ing job is nearly identical to the latency
profile of a mostly idle cluster without
tied requests. Tied requests allow the
workloads to be consolidated into a sin-
gle cluster, resulting in dramatic com-
puting cost reductions. In both Table 2
scenarios, the overhead of tied requests
in disk utilization is less than 1%, indi-
cating the cancellation strategy is effec-
tive at eliminating redundant reads.
An alternative to the tied-request and
hedged-request schemes is to probe re-
mote queues first, then submit the re-
quest to the least-loaded server.10 It can
be beneficial but is less effective than
submitting work to two queues simul-
taneously for three main reasons: load
levels can change between probe and re-
quest time; request service times can be
difficult to estimate due to underlying
system and hardware variability; and
clients can create temporary hot spots
by all clients picking the same (least-
loaded) server at the same time. The
Distributed Shortest-Positioning Time
First system9 uses another variation in
which the request is sent to one server
and forwarded to replicas only if the ini-
tial server does not have it in its cache
and uses cross-server cancellations.
Worth noting is this technique is not
restricted to simple replication but is
also applicable in more-complex coding
schemes (such as Reed-Solomon) where
a primary request is sent to the machine
with the desired data block, and, if no
response is received following a brief
delay, a collection of requests is issued
to a subset of the remaining replica-
tion group sufficient to reconstruct the
desired data, with the whole ensemble
forming a set of tied requests.
Note, too, the class of techniques de-
scribed here is effective only when the
phenomena that causes variability does
not tend to simultaneously affect mul-
tiple request replicas. We expect such
uncorrelated phenomena are rather
common in large-scale systems.
cross-Request
Long-Term adaptations
Here, we turn to techniques that are ap-
plicable for reducing latency variability
caused by coarser-grain phenomena
(such as service-time variations and
load imbalance). Although many sys-
tems try to partition data in such a way
that the partitions have equal cost, a
static assignment of a single partition
to each machine is rarely sufficient in
practice for two reasons: First, the per-
formance of the underlying machines
is neither uniform nor constant over
time, for reasons (such as thermal
throttling and shared workload inter-
ference) mentioned earlier. And second,
outliers in the assignment of items to
partitions can cause data-induced load
imbalance (such as when a particular
item becomes popular and the load for
its partition increases).
Micro-partitions. To combat imbal-
ance, many of Google’s systems gener-
ate many more partitions than there
are machines in the service, then do
dynamic assignment and load balanc-
ing of these partitions to particular ma-
chines. Load balancing is then a matter
of moving responsibility for one of these
small partitions from one machine to
another. With an average of, say, 20
partitions per machine, the system can
shed load in roughly 5% increments acontributed articles
february 2013 | vol. 56 | no. 2 | communicaTionS of The acm 79
in 1/20th the time it would take if the sys-
tem simply had a one-to-one mapping
of partitions to machines. The BigTable
distributed-storage system stores data
in tablets, with each machine managing
between 20 and 1,000 tablets at a time.
Failure-recovery speed is also improved
through micro-partitioning, since many
machines pick up one unit of work when
a machine failure occurs. This method
of using micro-partitions is similar to
the virtual servers notion as described
in Stoica12 and the virtual-processor-
partitioning technique in DeWitt et al.6
Selective replication. An enhance-
ment of the micro-partitioning scheme
is to detect or even predict certain items
that are likely to cause load imbalance
and create additional replicas of these
items. Load-balancing systems can then
use the additional replicas to spread
the load of these hot micro-partitions
across multiple machines without hav-
ing to actually move micro-partitions.
Google’s main Web search system uses
this approach, making additional cop-
ies of popular and important docu-
ments in multiple micro-partitions. At
various times in Google’s Web search
system’s evolution, it has also created
micro-partitions biased toward particu-
lar document languages and adjusted
replication of these micro-partitions
as the mix of query languages changes
through the course of a typical day.
Query mixes can also change abruptly,
as when, say, an Asian data-center out-
age causes a large fraction of Asian-lan-
guage queries to be directed to a North
American facility, materially changing
its workload behavior.
Latency-induced probation. By ob-
serving the latency distribution of re-
sponses from the various machines in
the system, intermediate servers some-
times detect situations where the sys-
tem performs better by excluding a par-
ticularly slow machine, or putting it on
probation. The source of the slowness is
frequently temporary phenomena like
interference from unrelated network-
ing traffic or a spike in CPU activity for
another job on the machine, and the
slowness tends to be noticed when the
system is under greater load. However,
the system continues to issue shadow
requests to these excluded servers, col-
lecting statistics on their latency so they
can be reincorporated into the service
when the problem abates. This situa-
tion is somewhat peculiar, as removal
of serving capacity from a live system
during periods of high load actually im-
proves latency.
Large information
Retrieval Systems
In large information-retrieval (IR) sys-
tems, speed is more than a performance
metric; it is a key quality metric, as re-
turning good results quickly is better
than returning the best results slowly.
Two techniques apply to such systems,
as well as other to systems that inher-
ently deal with imprecise results:
Good enough. In large IR systems,
once a sufficient fraction of all the leaf
servers has responded, the user may
be best served by being given slightly
incomplete (“good-enough”) results in
exchange for better end-to-end latency.
The chance that a particular leaf server
has the best result for the query is less
than one in 1,000 queries, odds further
reduced by replicating the most im-
portant documents in the corpus into
multiple leaf servers. Since waiting for
exceedingly slow servers might stretch
service latency to unacceptable levels,
Google’s IR systems are tuned to occa-
sionally respond with good-enough re-
sults when an acceptable fraction of the
overall corpus has been searched, while
being careful to ensure good-enough
results remain rare. In general, good-
enough schemes are also used to skip
nonessential subsystems to improve re-
sponsiveness; for example, results from
ads or spelling-correction systems are
easily skipped for Web searches if they
do not respond in time.
Canary requests. Another problem
that can occur in systems with very high
fan-out is that a particular request ex-
ercises an untested code path, causing
crashes or extremely long delays on
thousands of servers simultaneously. To
prevent such correlated crash scenarios,
some of Google’s IR systems employ
a technique called “canary requests”;
rather than initially send a request to
thousands of leaf servers, a root server
sends it first to one or two leaf servers.
The remaining servers are only queried
if the root gets a successful response
from the canary in a reasonable period
of time. If the server crashes or hangs
while the canary request is outstanding,
the system flags the request as poten-
tially dangerous and prevents further ex-ecution by not sending it to the remain-
ing leaf servers. Canary requests provide
a measure of robustness to back-ends in
the face of difficult-to-predict program-
ming errors, as well as malicious denial-
of-service attacks.
The canary-request phase adds only a
small amount of overall latency because
the system must wait for only a single
server to respond, producing much less
variability than if it had to wait for all
servers to respond for large fan-out re-
quests; compare the first and last rows
in Table 1. Despite the slight increase
in latency caused by canary requests,
such requests tend to be used for every
request in all of Google’s large fan-out
search systems due to the additional
safety they provide.
mutations
The techniques we have discussed so
far are most applicable for operations
that do not perform critical mutations
of the system’s state, which covers a
broad range of data-intensive services.
Tolerating latency variability for opera-
tions that mutate state is somewhat eas-
ier for a number of reasons: First, the
scale of latency-critical modifications
in these services is generally small. Sec-
ond, updates can often be performed
off the critical path, after responding
to the user. Third, many services can
be structured to tolerate inconsistent
update models for (inherently more
latency-tolerant) mutations. And, final-
ly, for those services that require con-
sistent updates, the most commonly
used techniques are quorum-based
algorithms (such as Lamport’s Paxos8
);
since these algorithms must commit to
only three to five replicas, they are in-
herently tail-tolerant.
hardware Trends and Their effects
Variability at the hardware level is likely
to be higher in the future due to more
aggressive power optimizations becom-
ing available and fabrication challenges
at deep submicron levels resulting in
device-level heterogeneity. Device het-
erogeneity combined with ever-increas-
ing system scale will make tolerating
variability through software techniques
even more important over time. For-
tunately, several emerging hardware
trends will increase the effectiveness
of latency-tolerating techniques. For
example, higher bisection bandwidth
in data-center networks and network-
interface optimizations that reduce
per-message overheads (such as remote
direct-memory access) will reduce the
cost of tied requests, making it more
likely that cancellation messages are re-
ceived in time to avoid redundant work.
Lower per-message overheads naturally
allow more fine-grain requests, contrib-
uting to better multiplexing and avoid-
ing head-of-line blocking effects.
conclusion
Delivering the next generation of com-
pute-intensive, seamlessly interactive
cloud services requires consistently
responsive massive-scale computing
systems that are only now beginning to
be contemplated. As systems scale up,
simply stamping out all sources of per-
formance variability will not achieve
such responsiveness. Fault-tolerant
techniques were developed because
guaranteeing fault-free operation be-
came infeasible beyond certain levels
of system complexity. Similarly, tail-
tolerant techniques are being devel-
oped for large-scale services because
eliminating all sources of variability is
also infeasible. Although approaches
that address particular sources of la-
tency variability are useful, the most
powerful tail-tolerant techniques re-
duce latency hiccups regardless of
root cause. These tail-tolerant tech-
niques allow designers to continue to
optimize for the common case while
providing resilience against uncom-
mon cases. We have outlined a small
collection of tail-tolerant techniques
that have been effective in several of
Google’s large-scale software systems.
Their importance will only increase as
Internet services demand ever-larger
and more complex warehouse-scale
systems and as the underlying hard-
ware components display greater per-
formance variability.
While some of the most powerful
tail-tolerant techniques require addi-
tional resources, their overhead can be
rather modest, often relying on existing
capacity already provisioned for fault-
tolerance while yielding substantial la-
tency improvements. In addition, many
of these techniques can be encapsu-
lated within baseline libraries and sys-
tems, and the latency improvements
often enable radically simpler applica-
tion-level designs. Besides enabling low
latency at large scale, these techniques
make it possible to achieve higher sys-
tem utilization without sacrificing ser-
vice responsiveness.
acknowledgments
We thank Ben Appleton, Zhifeng Chen,
Greg Ganger, Sanjay Ghemawat, Ali
Ghodsi, Rama Govindaraju, Lawrence
Greenfield, Steve Gribble, Brian Gus-
tafson, Nevin Heintze, Jeff Mogul, An-
drew Moore, Rob Pike, Sean Quinlan,
Gautham Thambidorai, Ion Stoica,
Amin Vahdat, and T.N. Vijaykumar for
their helpful feedback on earlier drafts
and presentations of this work. Numer-
ous people at Google have worked on
systems that use these techniques.